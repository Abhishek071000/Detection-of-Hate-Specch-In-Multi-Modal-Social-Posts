# Detection-of-Hate-Specch-In-Multi-Modal-Social-Posts
It has been observed in the past few years, multi-modal problems have been capable of attaining the interest of a large number of people. The core challenges faced in such problems are its representation, alignment, fusion, co-learning, and translation. The focus of this paper is on the analysis of multimodal memes for hate speech. On the evaluation of the dataset, we found out that the common statistics factors which were hateful initially became benign simply by unfolding the picture of the meme.
Correspondingly, a bulk of the multi-modal baselines gives hate speech more options. In order to deal with such issues, we discover the visible modality through the use of item detection and image captioning fashions to realize the “real caption” after which we integrate it with multi-modal illustration to carry out binary classification. The method challenges the benign textual content co-founders present in the dataset to enhance the
enactment. The second method that we use to test is to enhance the prediction with sentiment evaluation. It includes a unimodal sentiment to complement the features. Also we carry out in depth evaluation of the above methods stated, supplying compelling motives in want of the methodologies used.
